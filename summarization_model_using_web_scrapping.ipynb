{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0UHA5zxLiUs",
        "outputId": "88752a8a-270b-4971-c153-c4ff67fb2a5d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfE9hMblNWFm",
        "outputId": "436b6962-1d6e-4f37-e408-675d0855f996"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize('Hello, world!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83eRFdYZNaHL",
        "outputId": "5661a7c5-5a2c-4c6c-ce66-43d910658af4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'world', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV9X_m0pNqB7",
        "outputId": "c3fdbc9d-f799-49f9-d8aa-91a6340b2238"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvL4ZK53Nsc3",
        "outputId": "41348ccd-04b0-4098-8d49-49c836c08e17"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Web Scraping\n",
        "def scrape_website(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    text = ' '.join(p.text for p in soup.find_all('p'))\n",
        "    return text\n",
        "\n",
        "# Step 2: Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    filtered_words = [word for word in words if word not in stop_words and word.isalnum()]\n",
        "    return filtered_words\n",
        "\n",
        "# Step 3: Summarization\n",
        "def summarize(text, num_sentences=5):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = preprocess_text(text)\n",
        "    word_freq = Counter(words)\n",
        "    ranking = {sentence: sum(word_freq[word] for word in word_tokenize(sentence.lower())) for sentence in sentences}\n",
        "    summary = ' '.join(sorted(ranking, key=ranking.get, reverse=True)[:num_sentences])\n",
        "    return summary\n",
        "\n",
        "# Main\n",
        "url = 'https://timesofindia.indiatimes.com/india/ed-cannot-keep-filing-chargesheets-to-keep-accused-in-jail-supreme-court/articleshow/108645761.cmsarticle_text = scrape_website(url)'\n",
        "summary = summarize(article_text)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBdb6HEcLibd",
        "outputId": "58a61814-ba8d-49a0-e0de-8dc55e6befc4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Football\n",
            "                                    \n",
            " In the latest edition of Match Officials Mic'd Up, the audio of the officials' deliberations for Liverpool's penalty claim was released, revealing that they ruled in Man City's favour because Jeremy Doku and Alexis Mac Allister \"both come in high\" and contact is made with the ball Wednesday 20 March 2024 12:09, UK Please use Chrome browser for a more accessible video player  Sky Sports' Peter Smith has picked out two issues with the officials' explanation for not awarding Liverpool a penalty late on against Man City for Jeremy Doku's high challenge on Alexis Mac Allister. The audio of the officials' deliberations was released on Monday night in the latest edition of Match Officials Mic'd Up, revealing that they ruled in City's favour because \"they both come in high\" and Doku made contact with the ball. Two things for me: they say that Doku gets the ball, but on those super slo-mos you can see that Mac Allister gets there first. Well Doku has his boot up here and Mac Allister is trying to chest the ball, his feet have barely left the ground, if at all. Head of referees Howard Webb said the VAR was right not to overturn the decision, but Smith argues that the outcome contradicts previous guidance and fails to acknowledge Mac Allister's feet being on the ground.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to scrape webpage content\n",
        "def scrape_webpage(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "# Function to summarize text\n",
        "def summarize_text(text, num_sentences=3):\n",
        "    # This is where you'd implement your summarization logic.\n",
        "    # For simplicity, we're just taking the first few sentences.\n",
        "    sentences = text.split('. ')[:num_sentences]\n",
        "    return '. '.join(sentences) + '.'\n",
        "\n",
        "# Main function to summarize webpage with headings\n",
        "def summarize_webpage_with_headings(url):\n",
        "    soup = scrape_webpage(url)\n",
        "    summary = {}\n",
        "\n",
        "    # Assuming that the main content is within the 'article' tag\n",
        "    article = soup.find('article')\n",
        "    if not article:\n",
        "        return \"Article tag not found.\"\n",
        "\n",
        "    # Find all headings within the article tag\n",
        "    for heading in article.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
        "        # The next sibling of the heading tag can be the content associated with the heading\n",
        "        content = heading.find_next_sibling().get_text(separator=' ', strip=True)\n",
        "        summarized_content = summarize_text(content)\n",
        "        summary[heading.get_text()] = summarized_content\n",
        "\n",
        "    return summary\n",
        "\n",
        "# URL of the webpage to summarize\n",
        "url = 'https://sports.ndtv.com/football/convicted-of-rape-ex-brazil-star-dani-alves-freed-on-bail-for-1-million-euros-5275860'\n",
        "webpage_summary = summarize_webpage_with_headings(url)\n",
        "\n",
        "# Display the summary\n",
        "for heading, summarized_content in webpage_summary.items():\n",
        "    print(f\"### {heading}\\n- {summarized_content}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b4KwyKgOxDB",
        "outputId": "00086c08-4fd3-47f4-873f-4ea2262d2bc7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Convicted Of Rape, Ex-Brazil Star Dani Alves Freed On Bail For 1 Million Euros\n",
            "- The ruling came a day after his lawyer requested his release on the grounds that he had already served a quarter of his four-and-a-half-year sentence in pre-trial detention following his arrest in January 2023..\n",
            "\n",
            "### The ruling came a day after his lawyer requested his release on the grounds that he had already served a quarter of his four-and-a-half-year sentence in pre-trial detention following his arrest in January 2023.\n",
            "- Agence France-Presse Updated: March 20, 2024 04:54 PM IST Read Time: 2 min.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}